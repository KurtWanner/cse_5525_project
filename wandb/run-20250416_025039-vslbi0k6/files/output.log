  0%|                                                                                                                                                                                                                                                                                     | 0/3470 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
                                                                                                                                                                                                                                                                                                                   
{'loss': 6.6103, 'grad_norm': 10.117857933044434, 'learning_rate': 1.08e-06, 'epoch': 0.06}
{'loss': 5.8134, 'grad_norm': 5.91077184677124, 'learning_rate': 1.98e-06, 'epoch': 0.12}
{'loss': 5.3336, 'grad_norm': 3.1579630374908447, 'learning_rate': 2.88e-06, 'epoch': 0.17}
{'loss': 4.4169, 'grad_norm': 1.587039828300476, 'learning_rate': 2.9894736842105264e-06, 'epoch': 0.23}
{'loss': 4.2424, 'grad_norm': 0.1568455696105957, 'learning_rate': 2.9763157894736843e-06, 'epoch': 0.29}
{'loss': 3.9436, 'grad_norm': 0.0, 'learning_rate': 2.9675438596491228e-06, 'epoch': 0.35}
{'loss': 4.1374, 'grad_norm': 0.0, 'learning_rate': 2.9578947368421054e-06, 'epoch': 0.4}
{'loss': 4.1613, 'grad_norm': 0.0, 'learning_rate': 2.9447368421052633e-06, 'epoch': 0.46}
{'loss': 4.3594, 'grad_norm': 0.0, 'learning_rate': 2.9333333333333333e-06, 'epoch': 0.52}
{'loss': 4.3771, 'grad_norm': nan, 'learning_rate': 2.9210526315789475e-06, 'epoch': 0.58}
{'loss': 4.2882, 'grad_norm': nan, 'learning_rate': 2.910526315789474e-06, 'epoch': 0.63}
{'loss': 4.3215, 'grad_norm': 0.0, 'learning_rate': 2.899122807017544e-06, 'epoch': 0.69}
{'loss': 4.3451, 'grad_norm': 0.0, 'learning_rate': 2.8850877192982455e-06, 'epoch': 0.75}
{'loss': 4.505, 'grad_norm': nan, 'learning_rate': 2.8701754385964913e-06, 'epoch': 0.81}
{'loss': 3.8402, 'grad_norm': nan, 'learning_rate': 2.8596491228070176e-06, 'epoch': 0.86}
{'loss': 4.3808, 'grad_norm': nan, 'learning_rate': 2.8482456140350877e-06, 'epoch': 0.92}
{'loss': 4.2301, 'grad_norm': 0.0, 'learning_rate': 2.837719298245614e-06, 'epoch': 0.98}
{'loss': 3.9806, 'grad_norm': 0.0, 'learning_rate': 2.8263157894736845e-06, 'epoch': 1.04}
{'loss': 4.283, 'grad_norm': 0.0, 'learning_rate': 2.8140350877192982e-06, 'epoch': 1.1}
{'loss': 3.573, 'grad_norm': 0.0, 'learning_rate': 2.8043859649122804e-06, 'epoch': 1.15}
{'loss': 4.029, 'grad_norm': nan, 'learning_rate': 2.7921052631578946e-06, 'epoch': 1.21}
{'loss': 4.0544, 'grad_norm': nan, 'learning_rate': 2.7833333333333335e-06, 'epoch': 1.27}
{'loss': 3.5099, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.33}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.38}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.44}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.5}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.56}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.61}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.67}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.73}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.79}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.84}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.9}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 1.96}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 2.02}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 2.07}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 2.13}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 2.19}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 2.25}
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 2.780701754385965e-06, 'epoch': 2.31}
  File "/fs/scratch/PAS2912/rodent_translation/cse_5525_project/finetune.py", line 287, in <module>
    train()
  File "/fs/scratch/PAS2912/rodent_translation/cse_5525_project/finetune.py", line 271, in train
    trainer.train()
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/transformers/trainer.py", line 2560, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs, num_items_in_batch)
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/transformers/trainer.py", line 3782, in training_step
    self.accelerator.backward(loss, **kwargs)
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/accelerate/accelerator.py", line 2011, in backward
    self.scaler.scale(loss).backward(**kwargs)
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/torch/_tensor.py", line 522, in backward
    torch.autograd.backward(
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/torch/autograd/__init__.py", line 266, in backward
    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
KeyboardInterrupt
