  0%|                                                                                                                                                                                                                                                                                     | 0/3960 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
  9%|████████████████████████                                                                                                                                                                                                                                                   | 356/3960 [00:50<06:23,  9.40it/s]Traceback (most recent call last):
{'loss': 6.2301, 'grad_norm': 2.8333747386932373, 'learning_rate': 1.08e-05, 'epoch': 0.1}
{'loss': 4.52, 'grad_norm': 0.8810457587242126, 'learning_rate': 2.04e-05, 'epoch': 0.2}
{'loss': 3.924, 'grad_norm': 1.044961929321289, 'learning_rate': 2.99923273657289e-05, 'epoch': 0.3}
{'loss': 2.871, 'grad_norm': 1.8784161806106567, 'learning_rate': 2.9861892583120206e-05, 'epoch': 0.4}
{'loss': 1.5942, 'grad_norm': 1.3162130117416382, 'learning_rate': 2.973145780051151e-05, 'epoch': 0.51}
{'loss': 1.383, 'grad_norm': 0.5882501006126404, 'learning_rate': 2.9624040920716115e-05, 'epoch': 0.61}
{'loss': 1.4233, 'grad_norm': nan, 'learning_rate': 2.947826086956522e-05, 'epoch': 0.71}
{'loss': 1.3672, 'grad_norm': 0.0, 'learning_rate': 2.9378516624040923e-05, 'epoch': 0.81}
{'loss': 1.3428, 'grad_norm': 0.0, 'learning_rate': 2.9271099744245525e-05, 'epoch': 0.91}
{'loss': 1.4728, 'grad_norm': 0.0, 'learning_rate': 2.9132992327365727e-05, 'epoch': 1.01}
{'loss': 1.3517, 'grad_norm': 0.0, 'learning_rate': 2.9010230179028136e-05, 'epoch': 1.11}
{'loss': 1.4169, 'grad_norm': nan, 'learning_rate': 2.8895140664961636e-05, 'epoch': 1.21}
{'loss': 1.4403, 'grad_norm': 0.0, 'learning_rate': 2.879539641943734e-05, 'epoch': 1.31}
{'loss': 1.4161, 'grad_norm': 0.0, 'learning_rate': 2.8680306905370844e-05, 'epoch': 1.41}
{'loss': 1.4442, 'grad_norm': 0.0, 'learning_rate': 2.855754475703325e-05, 'epoch': 1.52}
{'loss': 1.4775, 'grad_norm': 0.0, 'learning_rate': 2.8419437340153455e-05, 'epoch': 1.62}
{'loss': 1.4405, 'grad_norm': 0.0, 'learning_rate': 2.8296675191815857e-05, 'epoch': 1.72}
  File "/fs/scratch/PAS2912/rodent_translation/cse_5525_project/finetune.py", line 287, in <module>
    train()
  File "/fs/scratch/PAS2912/rodent_translation/cse_5525_project/finetune.py", line 271, in train
    trainer.train()
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/transformers/trainer.py", line 2245, in train
    return inner_training_loop(
  File "/users/PAS2912/kurtwanner/miniconda3/envs/rodent/lib/python3.10/site-packages/transformers/trainer.py", line 2565, in _inner_training_loop
    and (torch.isnan(tr_loss_step) or torch.isinf(tr_loss_step))
KeyboardInterrupt
